{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from gensim import corpora, models, similarities\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# replace , . - ( ) / { } | _ to space\n",
    "pos = readdata('data/pos')\n",
    "pos = [replacePunctual(doc) for doc in pos]\n",
    "neg = readdata('data/neg')\n",
    "neg = [replacePunctual(doc) for doc in neg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalize to lower\n",
    "pos = [doc.lower() for doc in pos]\n",
    "neg = [doc.lower() for doc in neg]\n",
    "\n",
    "# Tokenize\n",
    "pos_tokens = [nltk.word_tokenize(doc) for doc in pos]\n",
    "neg_tokens = [nltk.word_tokenize(doc) for doc in neg]\n",
    "\n",
    "# lemmatizer\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "pos_tokens = [[wnl.lemmatize(word) for word in doc_token] for doc_token in pos_tokens]\n",
    "neg_tokens = [[wnl.lemmatize(word) for word in doc_token] for doc_token in neg_tokens]\n",
    "\n",
    "# Get training and testing data\n",
    "trainingX, trainingY, testingX, testingY = split(pos_tokens, neg_tokens)\n",
    "\n",
    "# Get valid words, including Freq(w) > 2 and non-stopwords and satisfy re = [a-zA-Z\\']*\n",
    "# Attention: only training data can be used \n",
    "validwords = genValidWords(trainingX)\n",
    "\n",
    "# Filter invalid words and use a flag to mark new words or invalid words\n",
    "trainingX = [[w if w in validwords else \"#\" for w in doc] for doc in trainingX]\n",
    "testingX = [[w if w in validwords else \"#\" for w in doc] for doc in testingX]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diction length is 15037\n"
     ]
    }
   ],
   "source": [
    "# use gensim to build dictionary\n",
    "dictionary = corpora.Dictionary(trainingX)\n",
    "numOfTerms = len(dictionary.token2id)\n",
    "print(\"diction length is %d\" % (numOfTerms))\n",
    "\n",
    "# print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert to BOW representation\n",
    "trainingBOW = [dictionary.doc2bow(doc) for doc in trainingX]\n",
    "testingBOW = [dictionary.doc2bow(doc) for doc in testingX]\n",
    "# print(trainingBOW[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.24568427101122933), (1, 0.06349722212399098), (2, 0.11326959562795012), (3, 0.02236969954698247), (4, 0.09105761863369047), (5, 0.004218948640747564), (6, 0.02885693845163563), (7, 0.017476616125373136), (8, 0.01318212224677471), (9, 0.05631703816311842), (10, 0.012557280651071962), (11, 0.017039134706569443), (12, 0.021086164576462316), (13, 0.0204606000135045), (14, 0.03544502893251888), (15, 0.02173786862550863), (16, 0.060032551059063015), (17, 0.152193395204837), (18, 0.042858005067506026), (19, 0.02339616395423827), (20, 0.012838996341179494), (21, 0.016479806044053826), (22, 0.022112324424430934), (23, 0.02849763714576092), (24, 0.03018449799096328), (25, 0.027277544375853585), (26, 0.04615680860676326), (27, 0.01978811906883763), (28, 0.0959530297369103), (29, 0.0031013595389703602), (30, 0.026867240610606663), (31, 0.05472021966641885), (32, 0.03435361875896172), (33, 0.03567782110663347), (34, 0.007060600343745879), (35, 0.05472021966641885), (36, 0.002895198159672448), (37, 0.02628073591726116), (38, 0.021495546095125932), (39, 0.045528809316845234), (40, 0.03414848699058273), (41, 0.012119293969910912), (42, 0.050038189028378574), (43, 0.03249590326417085), (44, 0.04073657747146598), (45, 0.03147768274124353), (46, 0.023837031160224592), (47, 0.07795324929178142), (48, 0.06635933556760237), (49, 0.09844848979782675), (50, 0.03116844139510013), (51, 0.010095283790053795), (52, 0.02707036115370158), (53, 0.02502901315089162), (54, 0.024861931854624666), (55, 0.033845919707675844), (56, 0.1094404393328377), (57, 0.04153380271983427), (58, 0.031635835672671454), (59, 0.010626621578031644), (60, 0.03554800751238456), (61, 0.02243505506846251), (62, 0.058205673585043874), (63, 0.03043346543196815), (64, 0.026668026474946753), (65, 0.053734481221213326), (66, 0.047540035705546296), (67, 0.008239903022026913), (68, 0.01296510543566263), (69, 0.01643406838106116), (70, 0.02243505506846251), (71, 0.06051717652850837), (72, 0.05472021966641885), (73, 0.11263407632623684), (74, 0.014527082527777038), (75, 0.11152376890340555), (76, 0.03355639351059343), (77, 0.030721228819363778), (78, 0.012233930015671218), (79, 0.02897952941841125), (80, 0.016180074183436997), (81, 0.017933391421191346), (82, 0.475400357055463), (83, 0.05631703816311842), (84, 0.005512930828615325), (85, 0.06051717652850837), (86, 0.06955297256100153), (87, 0.03897662464589071), (88, 0.05631703816311842), (89, 0.013481609528362172), (90, 0.04923251344829121), (91, 0.018104615415291164), (92, 0.017599207092148753), (93, 0.027185394178820693), (94, 0.08731076572609861), (95, 0.0973949500111088), (96, 0.05333699256763581), (97, 0.045528809316845234), (98, 0.026092388623188453), (99, 0.018411241229090654), (100, 0.05802271947930384), (101, 0.009800700456534535), (102, 0.03775653187598337), (103, 0.03692738421161856), (104, 0.02020252696083912), (105, 0.08875328690255324), (106, 0.04035985174467375), (107, 0.023541044858491223), (108, 0.006695558491427208), (109, 0.04153380271983427), (110, 0.03116844139510013), (111, 0.02230475378068111), (112, 0.025858299007059528), (113, 0.05472021966641885), (114, 0.03263908517968717), (115, 0.06203443024219428), (116, 0.020524588625194462), (117, 0.011098533297069519), (118, 0.010235676472489127), (119, 0.0063290859732252356), (120, 0.049136854202245867), (121, 0.009213789455172082), (122, 0.05472021966641885), (123, 0.04594659286539844), (124, 0.08278890695888465), (125, 0.06051717652850837), (126, 0.04035985174467375), (127, 0.05820717715073241), (128, 0.024861931854624666), (129, 0.007641334992913828), (130, 0.05472021966641885), (131, 0.029176754949251413), (132, 0.12103435305701674), (133, 0.11352234853736944), (134, 0.03834862535597268), (135, 0.035217353486487094), (136, 0.007516340223920966), (137, 0.050038189028378574), (138, 0.019586970694390846), (139, 0.053336052949893506), (140, 0.02167675449076308), (141, 0.10667398513527163), (142, 0.12699444424798195), (143, 0.025284799376163715), (144, 0.029229151516281564), (145, 0.11263407632623684), (146, 0.068713400515943), (147, 0.030016275529531507), (148, 0.06233688279020026), (149, 0.05771387690327126), (150, 0.32002195540581485), (151, 0.08479507489471928), (152, 0.058205673585043874), (153, 0.012209469990627313), (154, 0.05472021966641885), (155, 0.06051717652850837), (156, 0.050038189028378574), (157, 0.0678946772322719), (158, 0.01989017719282903), (159, 0.10667398513527163), (160, 0.039645167297908825), (161, 0.03804834880120925), (162, 0.0866797946803127), (163, 0.026569837912372416), (165, 0.04580893367729534), (166, 0.028014911259533808), (167, 0.026668026474946753), (168, 0.03308971782174932), (169, 0.052116899797728475), (170, 0.045528809316845234), (171, 0.01218506758642603), (172, 0.051094123770712124), (173, 0.04051102669281791), (174, 0.007516340223920966), (175, 0.022112324424430934), (176, 0.030868164840336696), (177, 0.006854594376810066), (178, 0.07089306499641483), (179, 0.039645167297908825), (180, 0.018361353873492237), (181, 0.06349722212399098), (182, 0.0723194267585676), (183, 0.011700260082831369), (184, 0.044936715836855934), (185, 0.01603768272088044), (186, 0.0244556517117989), (187, 0.049136854202245867), (188, 0.03116844139510013), (189, 0.019636892794834646), (190, 0.036409335477154116), (191, 0.03544502893251888), (192, 0.0012955396860413026), (193, 0.044936715836855934), (194, 0.03930550356339965), (195, 0.02339616395423827), (196, 0.005908434478177262), (197, 0.006805105495099883), (198, 0.07611445359903184), (199, 0.026376209549720874), (200, 0.004037492800992621), (201, 0.05333699256763581), (202, 0.017933391421191346), (203, 0.0035095371978089073), (204, 0.03375993111578267), (205, 0.0144055700987496), (206, 0.03263908517968717), (207, 0.008323509737328956), (208, 0.03544502893251888), (209, 0.042858005067506026), (210, 0.09231361721352652), (211, 0.04437664345127662), (212, 0.03930550356339965), (213, 0.036665121702426216), (214, 0.04073657747146598), (215, 0.009917530444465807), (216, 0.0154534487373175), (217, 0.02738271092171167), (218, 0.02738271092171167), (219, 0.027925181237137157), (220, 0.014166739559093562), (221, 0.010075382120545), (222, 0.05472021966641885), (223, 0.2188808786656754), (224, 0.02798232441349623), (225, 0.2377001785277315)]\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF\n",
    "tfidf = models.TfidfModel(trainingBOW)\n",
    "trainingTFIDF = tfidf[trainingBOW]\n",
    "testingTFIDF = tfidf[testingBOW]\n",
    "print(trainingTFIDF[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.084*\"!\" + 0.079*\"?\" + 0.070*\"alien\" + 0.067*\"\\'\" + 0.061*\"action\" + 0.060*\"wa\" + 0.059*\"movie\" + 0.058*\"bad\" + 0.056*\"scene\" + 0.055*\"really\"'),\n",
       " (1,\n",
       "  '-0.517*\"alien\" + 0.195*\"jackie\" + -0.182*\"ripley\" + -0.158*\"ship\" + 0.138*\"chan\" + -0.133*\"war\" + -0.130*\"jedi\" + 0.126*\"truman\" + -0.125*\"planet\" + -0.121*\"phantom\"'),\n",
       " (2,\n",
       "  '0.474*\"jackie\" + -0.469*\"truman\" + 0.351*\"chan\" + -0.216*\"carrey\" + 0.132*\"tarantino\" + 0.125*\"ordell\" + 0.104*\"brown\" + 0.080*\"martial\" + 0.079*\"tucker\" + -0.076*\"christof\"'),\n",
       " (3,\n",
       "  '-0.502*\"truman\" + 0.353*\"scream\" + -0.230*\"jackie\" + -0.227*\"carrey\" + 0.176*\"horror\" + -0.175*\"chan\" + 0.123*\"killer\" + 0.117*\"sidney\" + 0.098*\"craven\" + 0.098*\"williamson\"'),\n",
       " (4,\n",
       "  '0.333*\"alien\" + 0.304*\"truman\" + 0.304*\"scream\" + 0.214*\"jackie\" + 0.151*\"chan\" + 0.141*\"carrey\" + 0.138*\"horror\" + 0.132*\"ripley\" + -0.121*\"war\" + -0.114*\"phantom\"'),\n",
       " (5,\n",
       "  '0.310*\"scream\" + -0.263*\"alien\" + 0.211*\"phantom\" + 0.211*\"jedi\" + 0.200*\"lucas\" + 0.181*\"menace\" + 0.181*\"war\" + 0.166*\"truman\" + 0.144*\"horror\" + 0.141*\"jar\"'),\n",
       " (6,\n",
       "  '0.559*\"tarzan\" + 0.279*\"disney\" + 0.207*\"ape\" + 0.162*\"jane\" + 0.156*\"toy\" + 0.151*\"mulan\" + 0.127*\"animated\" + 0.123*\"jungle\" + 0.116*\"animation\" + 0.103*\"gorilla\"'),\n",
       " (7,\n",
       "  '-0.292*\"batman\" + -0.187*\"godzilla\" + 0.175*\"alien\" + 0.146*\"wedding\" + -0.141*\"schumacher\" + -0.135*\"robin\" + 0.132*\"sandler\" + -0.132*\"freeze\" + -0.132*\"!\" + 0.125*\"jackie\"'),\n",
       " (8,\n",
       "  '-0.305*\"sandler\" + -0.266*\"wedding\" + -0.163*\"godzilla\" + -0.160*\"julia\" + -0.159*\"singer\" + -0.148*\"!\" + -0.120*\"robbie\" + -0.104*\"barrymore\" + 0.104*\"libby\" + -0.104*\"adam\"'),\n",
       " (9,\n",
       "  '0.343*\"ship\" + 0.334*\"titanic\" + 0.198*\"rose\" + -0.198*\"alien\" + 0.172*\"cameron\" + 0.157*\"godzilla\" + 0.133*\"jack\" + 0.119*\"virus\" + 0.113*\"cal\" + 0.110*\"dicaprio\"')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsiNumOfTopics = 120\n",
    "# LSI\n",
    "lsi = models.LsiModel(trainingTFIDF, id2word=dictionary, num_topics=lsiNumOfTopics)\n",
    "trainingLSI = lsi[trainingTFIDF]\n",
    "testingLSI = lsi[testingTFIDF]\n",
    "\n",
    "trainingLSI = gensim.matutils.corpus2dense(trainingLSI, num_terms = lsiNumOfTopics).transpose()\n",
    "testingLSI = gensim.matutils.corpus2dense(testingLSI, num_terms = lsiNumOfTopics).transpose()\n",
    "lsi.print_topics(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(38,\n",
       "  '0.486*\"#\" + 0.011*\"\\'s\" + 0.008*\"film\" + 0.005*\"n\\'t\" + 0.004*\"wa\" + 0.004*\"one\" + 0.004*\"ha\" + 0.003*\"?\" + 0.003*\"movie\" + 0.003*\"like\"'),\n",
       " (9,\n",
       "  '0.250*\"#\" + 0.012*\"\\'s\" + 0.008*\"film\" + 0.004*\"wa\" + 0.003*\"n\\'t\" + 0.003*\"movie\" + 0.002*\"ha\" + 0.002*\"!\" + 0.002*\"character\" + 0.002*\"like\"'),\n",
       " (48,\n",
       "  '0.000*\"claiming\" + 0.000*\"uncaring\" + 0.000*\"marked\" + 0.000*\"dano\" + 0.000*\"aficionado\" + 0.000*\"conduct\" + 0.000*\"hershey\" + 0.000*\"rhetoric\" + 0.000*\"obscured\" + 0.000*\"obscurity\"'),\n",
       " (35,\n",
       "  '0.000*\"claiming\" + 0.000*\"uncaring\" + 0.000*\"marked\" + 0.000*\"dano\" + 0.000*\"aficionado\" + 0.000*\"conduct\" + 0.000*\"hershey\" + 0.000*\"rhetoric\" + 0.000*\"obscured\" + 0.000*\"obscurity\"'),\n",
       " (30,\n",
       "  '0.000*\"claiming\" + 0.000*\"uncaring\" + 0.000*\"marked\" + 0.000*\"dano\" + 0.000*\"aficionado\" + 0.000*\"conduct\" + 0.000*\"hershey\" + 0.000*\"rhetoric\" + 0.000*\"obscured\" + 0.000*\"obscurity\"'),\n",
       " (40,\n",
       "  '0.000*\"claiming\" + 0.000*\"uncaring\" + 0.000*\"marked\" + 0.000*\"dano\" + 0.000*\"aficionado\" + 0.000*\"conduct\" + 0.000*\"hershey\" + 0.000*\"rhetoric\" + 0.000*\"obscured\" + 0.000*\"obscurity\"'),\n",
       " (29,\n",
       "  '0.000*\"claiming\" + 0.000*\"uncaring\" + 0.000*\"marked\" + 0.000*\"dano\" + 0.000*\"aficionado\" + 0.000*\"conduct\" + 0.000*\"hershey\" + 0.000*\"rhetoric\" + 0.000*\"obscured\" + 0.000*\"obscurity\"'),\n",
       " (1,\n",
       "  '0.104*\"#\" + 0.003*\"\\'s\" + 0.003*\"film\" + 0.002*\"one\" + 0.002*\"wa\" + 0.002*\"movie\" + 0.001*\"n\\'t\" + 0.001*\"alien\" + 0.001*\"character\" + 0.001*\"ha\"'),\n",
       " (28,\n",
       "  '0.000*\"claiming\" + 0.000*\"uncaring\" + 0.000*\"marked\" + 0.000*\"dano\" + 0.000*\"aficionado\" + 0.000*\"conduct\" + 0.000*\"hershey\" + 0.000*\"rhetoric\" + 0.000*\"obscured\" + 0.000*\"obscurity\"'),\n",
       " (20,\n",
       "  '0.000*\"#\" + 0.000*\"\\'s\" + 0.000*\"film\" + 0.000*\"boogie\" + 0.000*\"night\" + 0.000*\"character\" + 0.000*\"scene\" + 0.000*\"anderson\" + 0.000*\"mr\" + 0.000*\"jack\"')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldaNumOfTopics = 50\n",
    "#LDA\n",
    "ldaModel = models.LdaModel(trainingBOW, id2word=dictionary, num_topics=ldaNumOfTopics)\n",
    "trainingLDA = ldaModel[trainingBOW]\n",
    "testingLDA = ldaModel[testingBOW]\n",
    "\n",
    "trainingLDA = gensim.matutils.corpus2dense(trainingLDA, num_terms = ldaNumOfTopics).transpose()\n",
    "testingLDA = gensim.matutils.corpus2dense(testingLDA, num_terms = ldaNumOfTopics).transpose()\n",
    "\n",
    "ldaModel.print_topics(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data Preparation\n",
    "trainingTFIDF_m = gensim.matutils.corpus2dense(trainingTFIDF, num_terms = numOfTerms).transpose()\n",
    "testingTFIDF_m = gensim.matutils.corpus2dense(testingTFIDF, num_terms = numOfTerms).transpose()\n",
    "\n",
    "trainingBOW_m = gensim.matutils.corpus2dense(trainingBOW, num_terms = numOfTerms).transpose()\n",
    "testingBOW_m = gensim.matutils.corpus2dense(testingBOW, num_terms = numOfTerms).transpose()\n",
    "trainingBOW_m = np.where(trainingBOW_m>1, trainingBOW_m, 1)\n",
    "testingBOW_m = np.where(testingBOW_m>1, testingBOW_m, 1)\n",
    "\n",
    "trainingY = np.array(trainingY)\n",
    "testingY = np.array(testingY)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "targetNames = ['0', '1']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('bad', 0.0083522523002254757),\n",
      " ('often', 0.0056484275080608774),\n",
      " (\"n't\", 0.0046324853457242032),\n",
      " ('life', 0.0044796634610995371),\n",
      " ('#', 0.0043827111477590126),\n",
      " ('could', 0.004156346726239369),\n",
      " ('great', 0.0041413602484206922),\n",
      " ('many', 0.0039758838831258064),\n",
      " ('love', 0.003706629040612106),\n",
      " ('?', 0.0034742759607361909),\n",
      " ('family', 0.0032571502195385951),\n",
      " ('small', 0.0031835848974424499),\n",
      " ('people', 0.0031572873207524567),\n",
      " ('man', 0.0029853527754344061),\n",
      " ('bit', 0.0029501901011737544),\n",
      " ('always', 0.0029479341989952735),\n",
      " ('wonderful', 0.0028538839180434041),\n",
      " ('also', 0.0028116114810701033),\n",
      " ('make', 0.0027514020546252358),\n",
      " ('scene', 0.0026555969307502049),\n",
      " ('seen', 0.0026317650221137621),\n",
      " ('ha', 0.0026254911402508224),\n",
      " ('obvious', 0.002528769808314864),\n",
      " ('several', 0.0025071993107617435),\n",
      " ('film', 0.0024898176925192347),\n",
      " ('men', 0.0024776334705439648),\n",
      " ('go', 0.0024467061618328378),\n",
      " ('even', 0.0024417032508519893),\n",
      " ('good', 0.0024178874115670379),\n",
      " ('well', 0.0024035926309774009),\n",
      " ('plot', 0.0023778793976933828),\n",
      " ('time', 0.0023548935098603205),\n",
      " ('fun', 0.0023451428977162197),\n",
      " ('different', 0.0023379622547057893),\n",
      " ('nothing', 0.0023273337800570268),\n",
      " ('movie', 0.0022696096135309649),\n",
      " ('perfect', 0.0022045349223530236),\n",
      " ('giving', 0.002100647230024654),\n",
      " ('find', 0.0020191805152946036),\n",
      " ('performance', 0.0019796224299698504),\n",
      " ('character', 0.0018953140977691289),\n",
      " ('supposed', 0.0018926777154927514),\n",
      " ('get', 0.0018771181252177499),\n",
      " ('stupid', 0.001876008388976146),\n",
      " ('doe', 0.0018674273548022109),\n",
      " ('making', 0.0018525687452347662),\n",
      " ('best', 0.001847756387186928),\n",
      " ('screen', 0.0018374430395745041),\n",
      " ('city', 0.0018367714810876429),\n",
      " ('like', 0.0018322412104377743),\n",
      " ('hollywood', 0.0018277854411904756),\n",
      " ('come', 0.0018274162587684187),\n",
      " (\"'\", 0.0017552396011195862),\n",
      " (\"'s\", 0.0017545495390612498),\n",
      " ('around', 0.0017365103156112231),\n",
      " ('yet', 0.0017298209089861515),\n",
      " ('without', 0.0017241107789649657),\n",
      " ('feel', 0.0017186392825050665),\n",
      " ('least', 0.0017174217464099389),\n",
      " ('worst', 0.0016883467719317241),\n",
      " ('town', 0.0016869769546699735),\n",
      " ('guy', 0.0016749044923474339),\n",
      " ('beautiful', 0.0016542592226593271),\n",
      " ('series', 0.001654152984290249),\n",
      " ('going', 0.0016459154711276538),\n",
      " ('action', 0.0016451979325606482),\n",
      " ('old', 0.001642580513126071),\n",
      " ('dramatic', 0.0016385387161579816),\n",
      " ('new', 0.0016223475978927406),\n",
      " ('better', 0.0016165260245543562),\n",
      " ('show', 0.0016161319654844132),\n",
      " ('frank', 0.0016117289618417623),\n",
      " ('country', 0.0016092581703326431),\n",
      " ('give', 0.0016087663272760066),\n",
      " ('star', 0.0015825335735589093),\n",
      " ('realistic', 0.0015804111098841594),\n",
      " ('child', 0.0015726376770257584),\n",
      " ('kid', 0.0015667859364920707),\n",
      " ('lead', 0.0015627518359185779),\n",
      " ('comedy', 0.0015552910225644055),\n",
      " ('wife', 0.0015533634167230776),\n",
      " ('excellent', 0.0015453276132199691),\n",
      " ('true', 0.0015384986187399505),\n",
      " ('year', 0.0015378601170012551),\n",
      " ('maybe', 0.0015246402465675293),\n",
      " ('dark', 0.0015241272075023084),\n",
      " ('fiction', 0.0014951944520320394),\n",
      " ('voice', 0.0014927226779089565),\n",
      " ('original', 0.0014915635502064314),\n",
      " ('history', 0.0014906792048986004),\n",
      " ('want', 0.0014817566380540719),\n",
      " ('much', 0.0014814853456010344),\n",
      " ('really', 0.0014717168137022801),\n",
      " ('wa', 0.0014705927652148474),\n",
      " ('would', 0.0014362301438327907),\n",
      " ('one', 0.0014361059540472557),\n",
      " ('ca', 0.0014219394749773139),\n",
      " ('damon', 0.0014169026432116053),\n",
      " ('boring', 0.0014082101604458545),\n",
      " ('way', 0.0014051105636177332)]\n"
     ]
    }
   ],
   "source": [
    "# chi2 feature importance\n",
    "from pprint import pprint\n",
    "id2token = {v: k for k, v in dictionary.token2id.items()}\n",
    "\n",
    "# from sklearn import feature_selection\n",
    "# chi2, pval = feature_selection.chi2(trainingBOW_m, trainingY)\n",
    "# sorted_chi2 = sorted(enumerate(chi2), key=lambda x: x[1], reverse = True)\n",
    "# pprint([(id2token[p[0]], p[1]) for (i,p) in enumerate(sorted_chi2) if i < 100 ])\n",
    "\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "clf = ExtraTreesClassifier()\n",
    "clf = clf.fit(trainingBOW_m, trainingY)\n",
    "sorted_fi = sorted(enumerate(clf.feature_importances_), key=lambda x: x[1], reverse = True)\n",
    "pprint([(id2token[p[0]], p[1]) for (i,p) in enumerate(sorted_fi) if i < 100 ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.78      0.86      0.82       283\n",
      "          1       0.86      0.78      0.82       314\n",
      "\n",
      "avg / total       0.82      0.82      0.82       597\n",
      "\n",
      "[[243  40]\n",
      " [ 68 246]]\n"
     ]
    }
   ],
   "source": [
    "# 1. LR for BOW\n",
    "clf_lr_tfidf = LogisticRegression(penalty='l2', tol=0.001)\n",
    "clf_lr_tfidf.fit(trainingBOW_m, trainingY)\n",
    "predicted_lr_tfidf = clf_lr_tfidf.predict(testingBOW_m)\n",
    "print(classification_report(testingY, predicted_lr_tfidf, target_names = targetNames))\n",
    "print(confusion_matrix(testingY, predicted_lr_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.77      0.84      0.80       283\n",
      "          1       0.85      0.77      0.81       314\n",
      "\n",
      "avg / total       0.81      0.81      0.81       597\n",
      "\n",
      "[[239  44]\n",
      " [ 72 242]]\n"
     ]
    }
   ],
   "source": [
    "# 2. LR for TF-IDF\n",
    "clf_lr_tfidf = LogisticRegression(penalty='l2', tol=0.001)\n",
    "clf_lr_tfidf.fit(trainingTFIDF_m, trainingY)\n",
    "predicted_lr_tfidf = clf_lr_tfidf.predict(testingTFIDF_m)\n",
    "print(classification_report(testingY, predicted_lr_tfidf, target_names = targetNames))\n",
    "print(confusion_matrix(testingY, predicted_lr_tfidf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.69      0.80      0.74       283\n",
      "          1       0.79      0.68      0.73       314\n",
      "\n",
      "avg / total       0.74      0.74      0.74       597\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. LR for LSI\n",
    "clf_lr_tfidf = LogisticRegression(penalty='l2', tol=0.001)\n",
    "clf_lr_tfidf.fit(trainingLSI, trainingY)\n",
    "# predicted_lr_tfidf = clf_lr_tfidf.predict(testingLSI)\n",
    "print(classification_report(testingY, predicted_lr_tfidf, target_names = targetNames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.48      0.90      0.63       283\n",
      "          1       0.58      0.12      0.20       314\n",
      "\n",
      "avg / total       0.53      0.49      0.41       597\n",
      "\n",
      "[[255  28]\n",
      " [275  39]]\n"
     ]
    }
   ],
   "source": [
    "# 4. LR for LDA\n",
    "clf_lr_tfidf = LogisticRegression(penalty='l2', tol=0.001)\n",
    "clf_lr_tfidf.fit(trainingLDA, trainingY)\n",
    "predicted_lr_tfidf = clf_lr_tfidf.predict(testingLDA)\n",
    "print(classification_report(testingY, predicted_lr_tfidf, target_names = targetNames))\n",
    "print(confusion_matrix(testingY, predicted_lr_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
